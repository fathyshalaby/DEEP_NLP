{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Word_embeedings.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph8s29r5pDdK"
      },
      "source": [
        "# Pre-Trained Word embeedings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhmLwqVRpDdN",
        "outputId": "90ca5db0-7d2b-4484-ae63-430397a3c931"
      },
      "source": [
        "# RUN ONCE load models + data\n",
        "#!python -m spacy download en_core_web_lg\n",
        "#!python -m spacy download en_core_web_md\n",
        "#import nltk\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "# get data via wget if it doesn't exist\n",
        "#!wget -nc https://bitbucket.org/omerlevy/hyperwords/raw/f5a01ea3e44c37c40dcb8d7b813ba3baf330eac0/testsets/analogy/msr.txt\n",
        "#!wget -nc https://bitbucket.org/omerlevy/hyperwords/raw/f5a01ea3e44c37c40dcb8d7b813ba3baf330eac0/testsets/ws/ws353.txt\n",
        "#!wget -nc https://bitbucket.org/omerlevy/hyperwords/raw/f5a01ea3e44c37c40dcb8d7b813ba3baf330eac0/testsets/ws/ws353_relatedness.txt\n",
        "#!wget -nc https://bitbucket.org/omerlevy/hyperwords/raw/f5a01ea3e44c37c40dcb8d7b813ba3baf330eac0/testsets/ws/ws353_similarity.txt\n",
        "!wget -nc http://öä.eu/sst5.data.txt\n",
        "# other option: load data via gdrive mounting\n",
        "# drive.mount('/gdrive')\n",
        "# %cd /gdrive/My Drive/sst5/<"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-20 07:28:59--  http://xn--4ca9a.eu/sst5.data.txt\n",
            "Resolving xn--4ca9a.eu (xn--4ca9a.eu)... 37.120.161.172\n",
            "Connecting to xn--4ca9a.eu (xn--4ca9a.eu)|37.120.161.172|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://xn--4ca9a.eu/sst5.data.txt [following]\n",
            "--2020-04-20 07:28:59--  https://xn--4ca9a.eu/sst5.data.txt\n",
            "Connecting to xn--4ca9a.eu (xn--4ca9a.eu)|37.120.161.172|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1337522 (1,3M) [text/plain]\n",
            "Saving to: ‘sst5.data.txt’\n",
            "\n",
            "sst5.data.txt       100%[===================>]   1,28M  4,17MB/s    in 0,3s    \n",
            "\n",
            "2020-04-20 07:29:00 (4,17 MB/s) - ‘sst5.data.txt’ saved [1337522/1337522]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtxqlFZjpDdY"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import spacy \n",
        "import sklearn\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import spearmanr\n",
        "mapl = lambda x, f: list(map(x, f))\n",
        "mapa = lambda x, f: np.array(list(map(x, f)))\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGK33_FepDdf",
        "outputId": "dad7c38e-ba27-4c8f-ab25-da8759840066"
      },
      "source": [
        "msr = pd.read_csv('msr.txt',delimiter='\\t',header=None); msr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>good</td>\n",
              "      <td>better</td>\n",
              "      <td>rough</td>\n",
              "      <td>rougher</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>better</td>\n",
              "      <td>good</td>\n",
              "      <td>rougher</td>\n",
              "      <td>rough</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>good</td>\n",
              "      <td>best</td>\n",
              "      <td>rough</td>\n",
              "      <td>roughest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>best</td>\n",
              "      <td>good</td>\n",
              "      <td>roughest</td>\n",
              "      <td>rough</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>best</td>\n",
              "      <td>better</td>\n",
              "      <td>roughest</td>\n",
              "      <td>rougher</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7995</td>\n",
              "      <td>sent</td>\n",
              "      <td>send</td>\n",
              "      <td>avoided</td>\n",
              "      <td>avoid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7996</td>\n",
              "      <td>send</td>\n",
              "      <td>sends</td>\n",
              "      <td>avoid</td>\n",
              "      <td>avoids</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7997</td>\n",
              "      <td>sends</td>\n",
              "      <td>send</td>\n",
              "      <td>avoids</td>\n",
              "      <td>avoid</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7998</td>\n",
              "      <td>sends</td>\n",
              "      <td>sent</td>\n",
              "      <td>avoids</td>\n",
              "      <td>avoided</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7999</td>\n",
              "      <td>sent</td>\n",
              "      <td>sends</td>\n",
              "      <td>avoided</td>\n",
              "      <td>avoids</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0       1         2         3\n",
              "0       good  better     rough   rougher\n",
              "1     better    good   rougher     rough\n",
              "2       good    best     rough  roughest\n",
              "3       best    good  roughest     rough\n",
              "4       best  better  roughest   rougher\n",
              "...      ...     ...       ...       ...\n",
              "7995    sent    send   avoided     avoid\n",
              "7996    send   sends     avoid    avoids\n",
              "7997   sends    send    avoids     avoid\n",
              "7998   sends    sent    avoids   avoided\n",
              "7999    sent   sends   avoided    avoids\n",
              "\n",
              "[8000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuICpN_NpDdq",
        "outputId": "31896952-91a8-45ef-ff65-a9517a435309"
      },
      "source": [
        "w353_similarity_data = pd.read_csv('ws353_similarity.txt',delimiter= '\\t',header=None); w353_similarity_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>tiger</td>\n",
              "      <td>cat</td>\n",
              "      <td>7.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>tiger</td>\n",
              "      <td>tiger</td>\n",
              "      <td>10.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>plane</td>\n",
              "      <td>car</td>\n",
              "      <td>5.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>train</td>\n",
              "      <td>car</td>\n",
              "      <td>6.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>television</td>\n",
              "      <td>radio</td>\n",
              "      <td>6.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>rooster</td>\n",
              "      <td>voyage</td>\n",
              "      <td>0.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>noon</td>\n",
              "      <td>string</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>chord</td>\n",
              "      <td>smile</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>professor</td>\n",
              "      <td>cucumber</td>\n",
              "      <td>0.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>king</td>\n",
              "      <td>cabbage</td>\n",
              "      <td>0.23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>203 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0         1      2\n",
              "0         tiger       cat   7.35\n",
              "1         tiger     tiger  10.00\n",
              "2         plane       car   5.77\n",
              "3         train       car   6.31\n",
              "4    television     radio   6.77\n",
              "..          ...       ...    ...\n",
              "198     rooster    voyage   0.62\n",
              "199        noon    string   0.54\n",
              "200       chord     smile   0.54\n",
              "201   professor  cucumber   0.31\n",
              "202        king   cabbage   0.23\n",
              "\n",
              "[203 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34y35j8_pDdt",
        "outputId": "fab4d290-088b-44f3-c81b-b35c485a1f2f"
      },
      "source": [
        "w353_relatedness_data =  pd.read_csv('ws353_relatedness.txt',delimiter='\\t',header=None); w353_relatedness_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>computer</td>\n",
              "      <td>keyboard</td>\n",
              "      <td>7.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>Jerusalem</td>\n",
              "      <td>Israel</td>\n",
              "      <td>8.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>planet</td>\n",
              "      <td>galaxy</td>\n",
              "      <td>8.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>canyon</td>\n",
              "      <td>landscape</td>\n",
              "      <td>7.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>OPEC</td>\n",
              "      <td>country</td>\n",
              "      <td>5.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>rooster</td>\n",
              "      <td>voyage</td>\n",
              "      <td>0.62</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>noon</td>\n",
              "      <td>string</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>chord</td>\n",
              "      <td>smile</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>professor</td>\n",
              "      <td>cucumber</td>\n",
              "      <td>0.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>king</td>\n",
              "      <td>cabbage</td>\n",
              "      <td>0.23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>252 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             0          1     2\n",
              "0     computer   keyboard  7.62\n",
              "1    Jerusalem     Israel  8.46\n",
              "2       planet     galaxy  8.11\n",
              "3       canyon  landscape  7.53\n",
              "4         OPEC    country  5.63\n",
              "..         ...        ...   ...\n",
              "247    rooster     voyage  0.62\n",
              "248       noon     string  0.54\n",
              "249      chord      smile  0.54\n",
              "250  professor   cucumber  0.31\n",
              "251       king    cabbage  0.23\n",
              "\n",
              "[252 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMvkfD4tpDdw"
      },
      "source": [
        "# Word Embedding Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2zRTfn1pDdw"
      },
      "source": [
        "### Pre-trained Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRs0wsjqpDdx"
      },
      "source": [
        "#### word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCwGZ53mpDdx",
        "outputId": "affd3c14-8a02-4a3e-a359-6606cb16f9d1"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")  #load in the model \n",
        "print(*nlp.pipeline, sep = \"\\n\")\n",
        "\n",
        "def tok_extract(line): \n",
        "    return [tok.vector for tok in line]\n",
        "\n",
        "def to_line(df):\n",
        "    # spacy tokenizes 's as a separate token -> the dimensions don't match up, so we have to remove it\n",
        "    # also it expects a space-separatated text, so we feed in the 8000x4 as 8000 lines of 4 space separated words\n",
        "    return [\" \".join(line).replace(\"'\", \"\") for line in df.to_numpy()]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('tagger', <spacy.pipeline.pipes.Tagger object at 0x1a1b01e890>)\n",
            "('parser', <spacy.pipeline.pipes.DependencyParser object at 0x1a1b141050>)\n",
            "('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x1a1b1410c0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpR8p-ZYpDd1",
        "outputId": "4ed40609-de4a-44d9-da0e-210bb0dd899e"
      },
      "source": [
        "# msr\n",
        "docl_msr_w = nlp.pipe(to_line(msr))\n",
        "w2v_msr = np.array(mapl(tok_extract, docl_msr_w)); w2v_msr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 4, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3awu_i1pDd2",
        "outputId": "678b496c-ed28-45ec-9e08-034a14294abf"
      },
      "source": [
        "# w353_relatedness\n",
        "docl_relat_w = nlp.pipe(to_line(w353_relatedness_data[[0,1]]))\n",
        "w2v_relatedness = np.array(mapl(tok_extract, docl_relat_w)); w2v_relatedness.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(252, 2, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp--ITHkpDd4",
        "outputId": "f1a5f393-db01-48c6-a850-4655394e622e"
      },
      "source": [
        "#w353_similarity\n",
        "docl_sim_w = nlp.pipe(to_line(w353_similarity_data[[0,1]]))\n",
        "w2v_similarity = np.array(mapl(tok_extract, docl_sim_w)); w2v_similarity.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(203, 2, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28Wnum7epDd6",
        "outputId": "d05453e5-87e2-4fd6-c8ce-4f49b7b41a29"
      },
      "source": [
        "words = \"man woman\"\n",
        "tokens = nlp(words) \n",
        "  \n",
        "for token in tokens: \n",
        "    # Printing the following attributes of each token. \n",
        "    # text: the word string, has_vector: if it contains \n",
        "    # a vector representation in the model,  \n",
        "    # vector_norm: the algebraic norm of the vector, \n",
        "    # is_oov: if the word is out of vocabulary. \n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov) \n",
        "  \n",
        "print(\"Similarity:\", tokens[0].similarity(tokens[1] )*10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man True 6.352939 False\n",
            "woman True 6.8987513 False\n",
            "Similarity: 7.40174412727356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-of8_yTpDd9"
      },
      "source": [
        "#### GloVe\n",
        "The en_core_web_lg model already has GloVe built in\n",
        "https://spacy.io/usage/vectors-similarity#custom-vectors-coverage\n",
        "\n",
        "Quote: *For instance, the en_vectors_web_lg model provides 300-dimensional GloVe vectors for over 1 million terms of English.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ie6W64zZpDd9",
        "outputId": "7029ec8f-ff01-4d57-dd39-5761a333736c"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_lg\")  #load in the model \n",
        "print(*nlp.pipeline, sep = \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('tagger', <spacy.pipeline.pipes.Tagger object at 0x1a1af0b810>)\n",
            "('parser', <spacy.pipeline.pipes.DependencyParser object at 0x1a51513e50>)\n",
            "('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x1a51513b40>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oJc8Md6pDeA",
        "outputId": "168264b2-4181-48c0-8e3d-3d0b7bb34dba"
      },
      "source": [
        "#msr\n",
        "docl_msr_glov = nlp.pipe(to_line(msr))\n",
        "glov_msr = np.array(mapl(tok_extract, docl_msr_glov)); glov_msr.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8000, 4, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvpsok1hpDeB",
        "outputId": "ad7fb3aa-5a8a-43d2-8c2a-08a8e3f5ecd5"
      },
      "source": [
        "# w353_relatedness, only take [0,1] as [2] is the scores\n",
        "docl_relat_glov = nlp.pipe(to_line(w353_relatedness_data[[0,1]]))\n",
        "glov_relatedness = np.array(mapl(tok_extract, docl_relat_glov)); glov_relatedness.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(252, 2, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPTJUcqapDeD",
        "outputId": "1466b91b-290e-4175-c1a2-fd2ae0cbacaa"
      },
      "source": [
        "#w353_similarity, only take [0,1] as [2] is the scores\n",
        "docl_sim_glov = nlp.pipe(to_line(w353_similarity_data[[0,1]]))\n",
        "glov_similarity = np.array(mapl(tok_extract, docl_sim_glov)); glov_similarity.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(203, 2, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRZlu6MepDeE",
        "outputId": "e1caf878-f6da-48b1-8749-18ea6d8bff6e"
      },
      "source": [
        "#sanity checker \n",
        "\n",
        "words = \"man woman\"\n",
        "tokens = nlp(words) \n",
        "  \n",
        "for token in tokens: \n",
        "    # Printing the following attributes of each token. \n",
        "    # text: the word string, has_vector: if it contains \n",
        "    # a vector representation in the model,  \n",
        "    # vector_norm: the algebraic norm of the vector, \n",
        "    # is_oov: if the word is out of vocabulary. \n",
        "    print(token.text, token.has_vector, token.vector_norm, token.is_oov) \n",
        "  \n",
        "token1, token2 = tokens[0], tokens[1] \n",
        "  \n",
        "print(\"Similarity:\", token1.similarity(token2)*10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "man True 6.352939 False\n",
            "woman True 6.8987513 False\n",
            "Similarity: 7.40174412727356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_fM0Ai1pDeI"
      },
      "source": [
        "# helper functions for similarity and analogy\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine(a, b):  # works identically to scipy.spatial.distance.cosine\n",
        "    return 1.0 - a @ b / (norm(a) * norm(b)) \n",
        "\n",
        "def three_cos_add(a1, a2, b1, b2):  # 3CosAdd # b2 needs to be vector in the model all \n",
        "    return cosine(b2, a2 - a1 + b1) # implementation from LevyA\n",
        "    #return cosine (b2, b1) - cosine (b2, a1) + cosine (b2, a2) # implementation from LevyB, chapter 6\n",
        "\n",
        "def three_cos_mul(a1, a2, b1, b2, epsilon=0.001):  # 3CosMul\n",
        "    return cosine(b2, b1) * cosine(b2, a2) / (cosine(b2, a1) + epsilon) # implementation from LevyA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghcq9GAepDeK"
      },
      "source": [
        "### Analogy Task 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVRexVLdpDeK",
        "outputId": "7d581feb-2c7e-4645-9a5b-2fe364301747"
      },
      "source": [
        "def analogy(data, method, debug_print=False): #analogy task function\n",
        "    total_score = 0\n",
        "    for id_correct, (a1, b1, a2, _) in enumerate(data):\n",
        "        #a1 = nlp(msr[0][id_correct]).vector\n",
        "        #b1 = nlp(msr[1][id_correct]).vector\n",
        "        #a2 = nlp(msr[2][id_correct]).vector\n",
        "        scores = [0]*len(data)\n",
        "        for id2, (_, _, _, b2) in enumerate(data):\n",
        "            #b2 = nlp(msr[3][id_correct]).vector\n",
        "            score = method(a1, b1, a2, b2)\n",
        "            scores[id2] = score\n",
        "        best_scoring = np.argmax(scores)\n",
        "        \n",
        "        if debug_print:\n",
        "            print(f\"Correct: {msr[0][id_correct]}:{msr[1][id_correct]} = {msr[2][id_correct]}:{msr[3][id_correct]  } - scores {scores[id_correct]}\")\n",
        "            print(f\"Best:    {msr[0][id_correct]}:{msr[1][id_correct]} = {msr[2][id_correct]}:{msr[3][best_scoring]} - scores {scores[best_scoring]}\\n\")\n",
        "        if np.argmax(scores) == id_correct:\n",
        "            total_score+=1\n",
        "    return total_score/len(data)\n",
        "\n",
        "#word2vec\n",
        "for data_name, data in [(\"w2v\", w2v_msr), (\"glove\", glov_msr)]:\n",
        "    for method in [three_cos_add, three_cos_mul]:\n",
        "        score = analogy(data[:16], three_cos_add, debug_print=True)\n",
        "        print(f\"score for {data_name} {method.__name__}: {score}\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct: good:better = rough:rougher - scores 0.465084969997406\n",
            "Best:    good:better = rough:meanest - scores 0.9205763563513756\n",
            "\n",
            "Correct: better:good = rougher:rough - scores 0.49284571409225464\n",
            "Best:    better:good = rougher:meanest - scores 0.9684790633618832\n",
            "\n",
            "Correct: good:best = rough:roughest - scores 0.48699039220809937\n",
            "Best:    good:best = rough:meaner - scores 0.8533295392990112\n",
            "\n",
            "Correct: best:good = roughest:rough - scores 0.47195178270339966\n",
            "Best:    best:good = roughest:meaner - scores 0.7584288716316223\n",
            "\n",
            "Correct: best:better = roughest:rougher - scores 0.5762052536010742\n",
            "Best:    best:better = roughest:meanest - scores 0.7005199790000916\n",
            "\n",
            "Correct: better:best = rougher:roughest - scores 0.6290794014930725\n",
            "Best:    better:best = rougher:mean - scores 0.9664024896919727\n",
            "\n",
            "Correct: good:better = mean:meaner - scores 0.6209504902362823\n",
            "Best:    good:better = mean:roughest - scores 0.8547815978527069\n",
            "\n",
            "Correct: better:good = meaner:mean - scores 0.7780361622571945\n",
            "Best:    better:good = meaner:heavy - scores 0.8782682567834854\n",
            "\n",
            "Correct: good:best = mean:meanest - scores 0.6516038179397583\n",
            "Best:    good:best = mean:rougher - scores 0.9043486416339874\n",
            "\n",
            "Correct: best:good = meanest:mean - scores 0.6463183164596558\n",
            "Best:    best:good = meanest:heavy - scores 0.9051701352000237\n",
            "\n",
            "Correct: best:better = meanest:meaner - scores 0.43241095542907715\n",
            "Best:    best:better = meanest:heavy - scores 0.9553603008389473\n",
            "\n",
            "Correct: better:best = meaner:meanest - scores 0.4580056667327881\n",
            "Best:    better:best = meaner:heavy - scores 1.0077006830833852\n",
            "\n",
            "Correct: good:better = heavy:heavier - scores 0.23409301042556763\n",
            "Best:    good:better = heavy:meanest - scores 1.009362993761897\n",
            "\n",
            "Correct: better:good = heavier:heavy - scores 0.2449955940246582\n",
            "Best:    better:good = heavier:meanest - scores 0.9798949528485537\n",
            "\n",
            "Correct: good:best = heavy:heaviest - scores 0.4385140538215637\n",
            "Best:    good:best = heavy:meaner - scores 0.9529063254594803\n",
            "\n",
            "Correct: best:good = heaviest:heavy - scores 0.3375677466392517\n",
            "Best:    best:good = heaviest:meanest - scores 1.0677358731627464\n",
            "\n",
            "score for w2v three_cos_add: 0.0\n",
            "\n",
            "\n",
            "Correct: good:better = rough:rougher - scores 0.465084969997406\n",
            "Best:    good:better = rough:meanest - scores 0.9205763563513756\n",
            "\n",
            "Correct: better:good = rougher:rough - scores 0.49284571409225464\n",
            "Best:    better:good = rougher:meanest - scores 0.9684790633618832\n",
            "\n",
            "Correct: good:best = rough:roughest - scores 0.48699039220809937\n",
            "Best:    good:best = rough:meaner - scores 0.8533295392990112\n",
            "\n",
            "Correct: best:good = roughest:rough - scores 0.47195178270339966\n",
            "Best:    best:good = roughest:meaner - scores 0.7584288716316223\n",
            "\n",
            "Correct: best:better = roughest:rougher - scores 0.5762052536010742\n",
            "Best:    best:better = roughest:meanest - scores 0.7005199790000916\n",
            "\n",
            "Correct: better:best = rougher:roughest - scores 0.6290794014930725\n",
            "Best:    better:best = rougher:mean - scores 0.9664024896919727\n",
            "\n",
            "Correct: good:better = mean:meaner - scores 0.6209504902362823\n",
            "Best:    good:better = mean:roughest - scores 0.8547815978527069\n",
            "\n",
            "Correct: better:good = meaner:mean - scores 0.7780361622571945\n",
            "Best:    better:good = meaner:heavy - scores 0.8782682567834854\n",
            "\n",
            "Correct: good:best = mean:meanest - scores 0.6516038179397583\n",
            "Best:    good:best = mean:rougher - scores 0.9043486416339874\n",
            "\n",
            "Correct: best:good = meanest:mean - scores 0.6463183164596558\n",
            "Best:    best:good = meanest:heavy - scores 0.9051701352000237\n",
            "\n",
            "Correct: best:better = meanest:meaner - scores 0.43241095542907715\n",
            "Best:    best:better = meanest:heavy - scores 0.9553603008389473\n",
            "\n",
            "Correct: better:best = meaner:meanest - scores 0.4580056667327881\n",
            "Best:    better:best = meaner:heavy - scores 1.0077006830833852\n",
            "\n",
            "Correct: good:better = heavy:heavier - scores 0.23409301042556763\n",
            "Best:    good:better = heavy:meanest - scores 1.009362993761897\n",
            "\n",
            "Correct: better:good = heavier:heavy - scores 0.2449955940246582\n",
            "Best:    better:good = heavier:meanest - scores 0.9798949528485537\n",
            "\n",
            "Correct: good:best = heavy:heaviest - scores 0.4385140538215637\n",
            "Best:    good:best = heavy:meaner - scores 0.9529063254594803\n",
            "\n",
            "Correct: best:good = heaviest:heavy - scores 0.3375677466392517\n",
            "Best:    best:good = heaviest:meanest - scores 1.0677358731627464\n",
            "\n",
            "score for w2v three_cos_mul: 0.0\n",
            "\n",
            "\n",
            "Correct: good:better = rough:rougher - scores 0.29812824726104736\n",
            "Best:    good:better = rough:meanest - scores 0.8105122745037079\n",
            "\n",
            "Correct: better:good = rougher:rough - scores 0.3159840703010559\n",
            "Best:    better:good = rougher:mean - scores 0.8913188725709915\n",
            "\n",
            "Correct: good:best = rough:roughest - scores 0.4654794931411743\n",
            "Best:    good:best = rough:meaner - scores 0.8689048141241074\n",
            "\n",
            "Correct: best:good = roughest:rough - scores 0.4205594062805176\n",
            "Best:    best:good = roughest:mean - scores 0.8449133634567261\n",
            "\n",
            "Correct: best:better = roughest:rougher - scores 0.3175368905067444\n",
            "Best:    best:better = roughest:mean - scores 0.8007203042507172\n",
            "\n",
            "Correct: better:best = rougher:roughest - scores 0.3701890707015991\n",
            "Best:    better:best = rougher:mean - scores 1.0262489672750235\n",
            "\n",
            "Correct: good:better = mean:meaner - scores 0.6880048513412476\n",
            "Best:    good:better = mean:roughest - scores 0.962751992046833\n",
            "\n",
            "Correct: better:good = meaner:mean - scores 0.8755826950073242\n",
            "Best:    better:good = meaner:mean - scores 0.8755826950073242\n",
            "\n",
            "Correct: good:best = mean:meanest - scores 0.7555533647537231\n",
            "Best:    good:best = mean:rougher - scores 0.9475164972245693\n",
            "\n",
            "Correct: best:good = meanest:mean - scores 0.7451743483543396\n",
            "Best:    best:good = meanest:heavy - scores 0.8057740330696106\n",
            "\n",
            "Correct: best:better = meanest:meaner - scores 0.3406527638435364\n",
            "Best:    best:better = meanest:heavy - scores 0.8557933866977692\n",
            "\n",
            "Correct: better:best = meaner:meanest - scores 0.3771269917488098\n",
            "Best:    better:best = meaner:mean - scores 1.0108987418934703\n",
            "\n",
            "Correct: good:better = heavy:heavier - scores 0.23409301042556763\n",
            "Best:    good:better = heavy:meanest - scores 0.9245846197009087\n",
            "\n",
            "Correct: better:good = heavier:heavy - scores 0.2449955940246582\n",
            "Best:    better:good = heavier:meanest - scores 0.8659467399120331\n",
            "\n",
            "Correct: good:best = heavy:heaviest - scores 0.3712402582168579\n",
            "Best:    good:best = heavy:meaner - scores 0.9533491022884846\n",
            "\n",
            "Correct: best:good = heaviest:heavy - scores 0.34658879041671753\n",
            "Best:    best:good = heaviest:meaner - scores 0.7600753605365753\n",
            "\n",
            "score for glove three_cos_add: 0.0625\n",
            "\n",
            "\n",
            "Correct: good:better = rough:rougher - scores 0.29812824726104736\n",
            "Best:    good:better = rough:meanest - scores 0.8105122745037079\n",
            "\n",
            "Correct: better:good = rougher:rough - scores 0.3159840703010559\n",
            "Best:    better:good = rougher:mean - scores 0.8913188725709915\n",
            "\n",
            "Correct: good:best = rough:roughest - scores 0.4654794931411743\n",
            "Best:    good:best = rough:meaner - scores 0.8689048141241074\n",
            "\n",
            "Correct: best:good = roughest:rough - scores 0.4205594062805176\n",
            "Best:    best:good = roughest:mean - scores 0.8449133634567261\n",
            "\n",
            "Correct: best:better = roughest:rougher - scores 0.3175368905067444\n",
            "Best:    best:better = roughest:mean - scores 0.8007203042507172\n",
            "\n",
            "Correct: better:best = rougher:roughest - scores 0.3701890707015991\n",
            "Best:    better:best = rougher:mean - scores 1.0262489672750235\n",
            "\n",
            "Correct: good:better = mean:meaner - scores 0.6880048513412476\n",
            "Best:    good:better = mean:roughest - scores 0.962751992046833\n",
            "\n",
            "Correct: better:good = meaner:mean - scores 0.8755826950073242\n",
            "Best:    better:good = meaner:mean - scores 0.8755826950073242\n",
            "\n",
            "Correct: good:best = mean:meanest - scores 0.7555533647537231\n",
            "Best:    good:best = mean:rougher - scores 0.9475164972245693\n",
            "\n",
            "Correct: best:good = meanest:mean - scores 0.7451743483543396\n",
            "Best:    best:good = meanest:heavy - scores 0.8057740330696106\n",
            "\n",
            "Correct: best:better = meanest:meaner - scores 0.3406527638435364\n",
            "Best:    best:better = meanest:heavy - scores 0.8557933866977692\n",
            "\n",
            "Correct: better:best = meaner:meanest - scores 0.3771269917488098\n",
            "Best:    better:best = meaner:mean - scores 1.0108987418934703\n",
            "\n",
            "Correct: good:better = heavy:heavier - scores 0.23409301042556763\n",
            "Best:    good:better = heavy:meanest - scores 0.9245846197009087\n",
            "\n",
            "Correct: better:good = heavier:heavy - scores 0.2449955940246582\n",
            "Best:    better:good = heavier:meanest - scores 0.8659467399120331\n",
            "\n",
            "Correct: good:best = heavy:heaviest - scores 0.3712402582168579\n",
            "Best:    good:best = heavy:meaner - scores 0.9533491022884846\n",
            "\n",
            "Correct: best:good = heaviest:heavy - scores 0.34658879041671753\n",
            "Best:    best:good = heaviest:meaner - scores 0.7600753605365753\n",
            "\n",
            "score for glove three_cos_mul: 0.0625\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icncpDWQpDeM"
      },
      "source": [
        "Even though the paper [*Improving Distributional Similaritywith Lessons Learned from Word Embeddings* (LevyA)](https://www.aclweb.org/anthology/Q15-1016.pdf) describes the formulas for 3CosAdd and references the paper [*Linguistic regularities in sparse and explicit word representations* (LevyB)](https://www.aclweb.org/anthology/W14-1618.pdf) and we checked with [an implementation of the paper on github from hrldcpr](https://github.com/hrldcpr/word-embeddings/blob/master/analogies.py), our implementation seems to perform exceptionally bad on our data.\n",
        "\n",
        "To show that we did all the steps to this point correctly, we print the intended value and its score, as well as the incorrectly best scornig values. We double-checked it by passing every [a1, a2, b1, b2] value of the original msr dataset through nlp(word).vector to verify that nothing went wrong inbetween. It seems that words like there is a bias for mean/meaner in the first 16 words, as they get picked overproportionally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDPYlcfYpDeN"
      },
      "source": [
        "### Similarity Tasks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BphmC3FypDeN",
        "outputId": "6bb8d96f-25b8-49d3-843f-050f23cc4b6b"
      },
      "source": [
        "#similarity task 1\n",
        "#word2vec\n",
        "\n",
        "def similarity(data,score):\n",
        "    results = []\n",
        "    i=0\n",
        "    for x, y in data:\n",
        "        cos_sim = np.dot(x,y) /(norm(x)*norm(y))\n",
        "        results.append((cos_sim,score[i]))\n",
        "        i+=1\n",
        "    actual,expected =zip(*results)  \n",
        "    return spearmanr(actual, expected)\n",
        "\n",
        "for test_name, d0, d1 in [\n",
        "    (\"w2v similarity  \", w2v_similarity, w353_similarity_data[2]), \n",
        "    (\"glove similarity\", glov_similarity,w353_similarity_data[2]),\n",
        "    (\"w2v relatedness \", w2v_relatedness,w353_relatedness_data[2]),\n",
        "    (\"glov relatedness\", glov_relatedness,w353_relatedness_data[2])]:\n",
        "    score = similarity(d0, d1)\n",
        "    print(f\"{test_name}\\tcorrelation: {score.correlation}\\tpvalue: {score.pvalue}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w2v similarity  \tcorrelation: 0.7773731113888416\tpvalue: 2.4665099782450488e-42\n",
            "glove similarity\tcorrelation: 0.8015441627816334\tpvalue: 8.958292823931727e-47\n",
            "w2v relatedness \tcorrelation: 0.6435221232854792\tpvalue: 7.371981209320379e-31\n",
            "glov relatedness\tcorrelation: 0.644302760922754\tpvalue: 5.940670174531038e-31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItNx0gfKpDeO"
      },
      "source": [
        "In this task we took the results of embeededings which we got by running our data through the spacy models and extracting the embeedings which we get by the two methods(Word2vec and GloVe) and ran a similarity evaluation of the embeedings to see how the two embeedings differ from each other.\n",
        "\n",
        "For the expirement we ran a simple cosine similarity test which told us how similar word1 is to word2 and compared it to the human score. \n",
        "\n",
        "The result of our expirement is that we see that GloVe achives a higher correlation, however that is also due to the model which uses GloVe uses a larger corpus. As mentioned in the paper a larger corpus would allow for higher scores to be achieved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXflnXfPpDeP"
      },
      "source": [
        "##Document Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILnH-DjKpDeP",
        "outputId": "cec84f3b-cba7-4656-d7ba-932ec6cbb1ad"
      },
      "source": [
        "df = pd.read_csv('sst5.data.txt'); df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Reno himself can take credit for most of the m...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Despite the film 's shortcomings , the stories...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>Despite its dry wit and compassion , the film ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>The central character is n't complex enough to...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>Rifkin no doubt fancies himself something of a...</td>\n",
              "      <td>-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11839</td>\n",
              "      <td>11839</td>\n",
              "      <td>Just as moving , uplifting and funny as ever .</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11840</td>\n",
              "      <td>11840</td>\n",
              "      <td>Davis ... is so enamored of her own creation t...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11841</td>\n",
              "      <td>11841</td>\n",
              "      <td>An exhilarating futuristic thriller-noir , Min...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11842</td>\n",
              "      <td>11842</td>\n",
              "      <td>I got a headache watching this meaningless dow...</td>\n",
              "      <td>-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11843</td>\n",
              "      <td>11843</td>\n",
              "      <td>If you dig on David Mamet 's mind tricks ... r...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11844 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_id                                               text  label\n",
              "0                0  Reno himself can take credit for most of the m...      1\n",
              "1                1  Despite the film 's shortcomings , the stories...      1\n",
              "2                2  Despite its dry wit and compassion , the film ...     -1\n",
              "3                3  The central character is n't complex enough to...      0\n",
              "4                4  Rifkin no doubt fancies himself something of a...     -2\n",
              "...            ...                                                ...    ...\n",
              "11839        11839     Just as moving , uplifting and funny as ever .      1\n",
              "11840        11840  Davis ... is so enamored of her own creation t...     -1\n",
              "11841        11841  An exhilarating futuristic thriller-noir , Min...      2\n",
              "11842        11842  I got a headache watching this meaningless dow...     -2\n",
              "11843        11843  If you dig on David Mamet 's mind tricks ... r...      2\n",
              "\n",
              "[11844 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9iJo4AQpDeQ"
      },
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "X = np.array(df[\"text\"])\n",
        "Y = np.array(df[\"label\"])\n",
        "\n",
        "# pass the whole X through the NLP, even if it's just for stemming + tokenization\n",
        "docl = list(nlp.pipe(X)) # returns just an iterator -> make list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hobr9iyGpDeS"
      },
      "source": [
        "# as everything above here takes forever, cell split here so we never have to recalculate docl\n",
        "stem_doc = mapl(lambda line: [tok.lemma_ for tok in line], docl)\n",
        "\n",
        "# split in train and test\n",
        "text_train, text_test, label_train, label_test = train_test_split(docl,Y,test_size = 0.2,random_state = 42)\n",
        "\n",
        "# generate a vocabulary just from text_train\n",
        "vocab = sorted(set(flatten(stem_doc)))\n",
        "vocab_opt = {k:v for k,v in enumerate(vocab)} # dicts are optimized for lookup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj28hiaWpDeT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKPt8gK-pDeT",
        "outputId": "1da1efc2-2cfd-406e-c82e-bfadfe6076d8"
      },
      "source": [
        "def doc2vec(tokens):\n",
        "    return np.mean([tok.vector for tok in tokens], axis=0)\n",
        "\n",
        "text_train_vec = mapa(doc2vec, text_train)\n",
        "text_test_vec = mapa(doc2vec, text_test)\n",
        "text_train_vec.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9475, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYcHA60fpDeV"
      },
      "source": [
        "*Incorporating IDF. In this experiment, we extend the method to create document vectors by also\n",
        "taking into account the Inverse Document Frequency (IDF) of terms. To do it, calculate and store\n",
        "the IDF values of words given the collection of the classification task. The document embedding\n",
        "is then defined as follows:\n",
        "<br><br>\n",
        "vd = weighted average of vt_i with weight idf(ti)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLJYqi84pDeV",
        "outputId": "940c32ef-4816-4d11-c74e-359872727d22"
      },
      "source": [
        "docf = Counter(flatten(stem_doc)) # count of all stemmed tokens\n",
        "docf = defaultdict(lambda: np.inf, docf) # to ignore OOB words -> defaultscore = inf -> idf = 0\n",
        "\n",
        "def doc2idfvec(tokens): # if a token appears N times, it will be added N times\n",
        "    return np.mean([tok.vector / docf[tok.text] for tok in tokens], axis=0)\n",
        "\n",
        "text_train_idfvec = mapa(doc2idfvec, text_train)\n",
        "text_test_idfvec = mapa(doc2idfvec, text_test)\n",
        "text_train_idfvec.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9475, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmEYnrV1pDeX",
        "outputId": "208dea35-e8c2-480f-9d13-e7be2bc2d505"
      },
      "source": [
        "print(\"Dtype\\tClassifier\\ttrain_pred\\ttest_pred\")\n",
        "for data_name, train, test in [[\"AVG\", text_train_vec, text_test_vec], [\"IDF\", text_train_idfvec, text_test_idfvec]]:\n",
        "    for classifier in [RandomForestClassifier(random_state=42, n_estimators=100),\n",
        "                      LinearSVC()]:\n",
        "        fit = classifier.fit(train, label_train)\n",
        "        \n",
        "        pred_train = classifier.predict(train)\n",
        "        acc_train = accuracy_score(label_train, pred_train)\n",
        "        \n",
        "        pred_test = classifier.predict(test)\n",
        "        acc_test = accuracy_score(label_test, pred_test)\n",
        "        \n",
        "        cname = str(classifier).split(\"(\")[0][:12]\n",
        "        print(data_name, cname, \"{:.7f}\".format(acc_train), \"{:.7f}\".format(acc_test), sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dtype\tClassifier\ttrain_pred\ttest_pred\n",
            "AVG\tRandomForest\t1.0000000\t0.3862389\n",
            "AVG\tLinearSVC\t0.4909763\t0.4221190\n",
            "IDF\tRandomForest\t0.9977836\t0.3195441\n",
            "IDF\tLinearSVC\t0.3856464\t0.3326298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnBbP0DKpDeY"
      },
      "source": [
        "\n",
        "\n",
        " TF-IDF (and therefore also idf) performs significantly worse in the validation (test_pred) - this is why most libs like spacy internally use plain averages (as we showed in our Assignment1)\n",
        "\n",
        "Also, random forest massively overfits for cases like this (accuracy on train data is 99-100%), and predictors that take the geometry and proximity of the embedding space into account like SVMs or KNN perform rather good."
      ]
    }
  ]
}